{
    "capsulename": "Linear regressiondddd",
    "desc": "In statistics, linear regression is a linear approach to modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression.[1] This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.[2]",
    "topic0": "Introduction",
    "para1": "Given a data set {\\displaystyle \\{y_{i},\\,x_{i1},\\ldots ,x_{ip}\\}_{i=1}^{n}}\\{y_{i},\\,x_{i1},\\ldots ,x_{ip}\\}_{i=1}^{n} of n statistical units, a linear regression model assumes that the relationship between the dependent variable y and the p-vector of regressors x is linear. This relationship is modeled through a disturbance term or error variable \u03b5 \u2014 an unobserved random variable that adds \"noise\" to the linear relationship between the dependent variable and regressors. Thus the model takes the form",
    "subtopic2": "Assumptions",
    "para3": "Standard linear regression models with standard estimation techniques make a number of assumptions about the predictor variables, the response variables and their relationship. Numerous extensions have been developed that allow each of these assumptions to be relaxed (i.e. reduced to a weaker form), and in some cases eliminated entirely. Generally these extensions make the estimation procedure more complex and time-consuming, and may also require more data in order to produce an equally precise model.",
    "subtopic4": "Assumptions",
    "imageresquepic5": "static\\userarticlePzUAUMAcUi3OoxWY\\vWSQZM31sFg0LmN8e55YHqPXYFiYEmOH\\YBnWmAAfsrimage\\biNsitJmv6image7Capture.JPG",
    "para6": "This means that the mean of the response variable is a linear combination of the parameters (regression coefficients) and the predictor variables. Note that this assumption is much less restrictive than it may at first seem. Because the predictor variables are treated as fixed values (see above), linearity is really only a restriction on the parameters. The predictor variables themselves can be arbitrarily transformed, and in fact multiple copies of the same underlying predictor variable can be added, each one transformed differently. This technique is used, for example, in polynomial regression, which uses linear regression to fit the response variable as an arbitrary polynomial function (up to a given rank) of a predictor variable. With this much flexibility, models such as polynomial regression often have \"too much power\", in that they tend to overfit the data. As a result, some kind of regularization must typically be used to prevent unreasonable solutions coming out of the estimation process. Common examples are ridge regression and lasso regression. Bayesian linear regression can also be used, which by its nature is more or less immune to the problem of overfitting. (In fact, ridge regression and lasso regression can both be viewed as special cases of Bayesian linear regression, with particular types of prior distributions placed on the regression coefficients.)",
    "code7": "dkjhzgvdjkhdxcjhvg"
}